<!DOCTYPE html>
<html>
<head>
  <!-- <meta name="viewport" content="width=device-width, initial-scale=1.0" charset='utf-8'.> -->
  <meta charset='utf8'>
  <style >.hogehoge {
    padding: 20px;
    margin: 20px;
  }
  /* .wrapper{
  width: 100%;
  height: 100%;
} */
  </style>
  <script async src="https://docs.opencv.org/4.x/opencv.js" type="text/javascript" defer></script>
</head>
<body>
  
  <div class="camera">
    <video id="video" autoplay playsinline></video>
    
    <!-- <video id="test" width="500" height="400" controls></video> -->
</div><br>
<canvas id="movie" ></canvas>
<canvas id="canvas"></canvas>
<p id="text"></p>



<!-- <button class="hogehoge" type="button" id="start">start</button> -->
<!-- <button class="hogehoge" type="button" id="stop">stop</button> -->
<!-- <botton type="button" id="download">dl</botton> -->



<script>
var constrains = { 
    // video: {facingMode:{ exact:  "user"} } 
    video: {facingMode:{ exact:  "environment"} } 

  }


  let prevGray = null;
  let prevPoints = null;
  let startGray = null;
  let startPoints = null;
// 映像・音声を取得するかの設定

  var downloaddata = []
  let str = 0;

  var curSTREAM = null

  window.addEventListener('load' , function(){
    // var startbutton = document.getElementById('start')
  // var stopbutton  = document.getElementById('stop')
  // var downloadbutton = document.getElementById('download')
  var video = document.getElementById('video')// 適当にvideoタグのオブジェクトを取得
  

  var movie = document.getElementById("movie")
  var canvas = document.getElementById("canvas")

  var text = document.getElementById("text")

// var flag=0;
  const ctx = movie.getContext("2d")
  const cty = canvas.getContext("2d")
//   if (!(( nvUA.indexOf("Safari") != -1 ) && !(( nvUA.indexOf("Chrome") != -1 ))))
// {
  var Mediastream = movie.captureStream()   //iphon not
// }else{
//   flag=1;
// }



video.width  = window.innerWidth
video.height = window.innerHeight
  movie.width  = window.innerWidth
  movie.height = window.innerHeight
  
  video.style = 'display:none'
  video.muted = 'true'
  movie.style = 'display:none'
  
  // console.log(video.videoHeight)




  // constrains.video.facingMode ={ exact: "environment" };
    // まずはユーザーのカメラ・マイクへのアクセスを実施
    function onOpenCvReady() {
      console.log("OpenCV.jsが準備完了");
    console.log(constrains)
    navigator.mediaDevices.getUserMedia(constrains)
  .then(function (stream) {
    curSTREAM = stream
    video.srcObject = stream // streamはユーザーのカメラとマイクの情報で、これをvideoの入力ソースにする
    video.play()
    const videoTrack = stream.getVideoTracks()[0];

    // ビデオトラックの設定を取得
    const settings = videoTrack.getSettings();

    // カメラの解像度を取得
    video.width  = settings.width;
    video.height = settings.height;
    // video.width  = 400;
    // video.height = 300;

    console.log(video.height)
    movie.height = movie.width *video.height /video.width
    canvas.width = movie.width
    canvas.height = movie.height
    movie.width = 400;
    movie.height = movie.width *video.height /video.width

    text.textContent = settings.width + ":" + settings.height;


    function drawCameraToCanvas() {
          // 現在のフレームを<canvas>に描画
          ctx.drawImage(video, 0, 0, video.width, video.height, 0, 0, movie.width, movie.height);

          let src = cv.imread(movie);
          // let cav = cv.imread(movie);
          let gray = new cv.Mat();
        const dst = src.clone();
        
        // グレースケールに変換（特徴点検出に適した前処理）
        cv.cvtColor(src, gray, cv.COLOR_RGBA2GRAY);

        let equalized = new cv.Mat();
        cv.equalizeHist(gray, equalized);

        // ORB検出器を作成
        // let orb = new cv.ORB();
        // let keypoints = new cv.KeyPointVector();
        // let descriptors = new cv.Mat();

        // // 特徴点を検出
        // orb.detectAndCompute(equalized, new cv.Mat(), keypoints, descriptors);

        // for (let i = 0; i < keypoints.size(); i++) {
        // const point = keypoints.get(i).pt;
        // console.log(`特徴点 ${i}: x=${point.x}, y=${point.y}`);
        // }

        // // 特徴点を画像に描画
        // cv.drawKeypoints(src, keypoints, dst);

        if (startGray === null) {
          prevGray = equalized.clone();
          prevPoints = new cv.Mat();
          cv.goodFeaturesToTrack(prevGray, prevPoints, 100, 0.3, 7);
          startGray = prevGray.clone();
          startPoints = prevPoints.clone();
        } else{
          const nextPoints = new cv.Mat();
          const status = new cv.Mat();
          const err = new cv.Mat();
          cv.calcOpticalFlowPyrLK(startGray, equalized, startPoints, nextPoints, status, err);

          for (let i = 0; i < nextPoints.rows; i++) {
            if (status.data[i] === 1) {
              const pt1 = new cv.Point(prevPoints.data32F[i * 2], prevPoints.data32F[i * 2 + 1]);
              // const pt1 = new cv.Point(startPoints.data32F[i * 2], startPoints.data32F[i * 2 + 1]);
              const pt2 = new cv.Point(nextPoints.data32F[i * 2], nextPoints.data32F[i * 2 + 1]);
              cv.line(dst, pt1, pt2, [0, 255, 0, 255], 2);
              cv.circle(dst, pt2, 3, [0, 0, 255, 255], -1);
            }
          }

          startGray.delete();
          startPoints.delete();
          startGray = equalized.clone();
          startPoints = nextPoints.clone();

          // prevGray.delete();
          // prevPoints.delete();
          nextPoints.delete();
          status.delete();
          err.delete();
        }

        // 結果をCanvasに描画
        cv.imshow(movie, dst);

        cty.drawImage(movie, 0, 0, movie.width, movie.height, 0, 0, canvas.width, canvas.height);

        // メモリの解放
        src.delete();
        gray.delete();
        dst.delete();
        equalized.delete();
        // keypoints.delete();
        // descriptors.delete();
        // orb.delete();

          // 再度次のフレームを描画するためにrequestAnimationFrameを使う
          requestAnimationFrame(drawCameraToCanvas);
        }

        // 映像の準備ができたら描画開始
        video.onplaying = () => {
          drawCameraToCanvas();
        };
        
    // var vdata = stream.getVideoTracks()
    // var videodata =vdata.getSettings().facingMode
    // console.log(videodata)
    
  //   const mime = MediaRecorder.isTypeSupported("video/webm; codecs=vp9") ? "video/webm; codecs=vp9" : "video/webm"
  //   // console.log(mine)
    
  //   recorder = new MediaRecorder(/*(flag)?stream:*/Mediastream, { mimeType: mime }) // 映像の入力ソースをユーザーのデバイスから取得
  //   recorder.ondataavailable = function (e) {
  //     // var testvideo = document.getElementById('test')
  //     // testvideo.setAttribute('controls', '')
  //     // testvideo.setAttribute('width', 500)
  //     // testvideo.setAttribute('height', 400)
  //     // var outputdata = window.URL.createObjectURL(e.data) // videoタグが扱えるように、記録データを加工
  //     // testvideo.src = outputdata // テスト用のビデオのソースに記録データを設置
  //     if(e.data){
  //     downloaddata.push(e.data)
      console.log("dl")
  //     }
  //   }
    
  })

  .catch(function(err) {
      console.log("An error occured! " + err)
  })
}
cv.onRuntimeInitialized = onOpenCvReady;

//   startbutton.addEventListener('click', function(ev){
    
//     recorder.start(10000)
//     ev.preventDefault()
//   }, false);

//   stopbutton.addEventListener('click', function(ev) {
//     recorder.stop()
    
//     setTimeout(() => {
//       console.log(downloaddata)
//       var blob = new Blob(downloaddata, { type:  "video/webm"})// 録画ファイルをblob形式に出力
//       var url = window.URL.createObjectURL(blob) // データにアクセスするためのURLを作成
//       downloaddata = []
//       var a = document.createElement('a') // download属性を持ったaタグをクリックするとダウンロードができるので、それをシミュレートする
//       document.body.appendChild(a)
//       a.style = 'display:none'
//       a.href = url;
//       a.download =  'test' + str + '.webm'
//       str++
//       a.click()
//       window.URL.revokeObjectURL(url)
//     }, 0);

// //     navigator.mediaDevices.getUserMedia({ 
// //     video: true, 
// //     audio: true, 
// //     facingMode:{ exact: "environment" }
// //   })
// //   .then(function (stream0) {
// //     curSTREAM = stream0
// //     video.srcObject = stream0 // streamはユーザーのカメラとマイクの情報で、これをvideoの入力ソースにする
// //     video.play()
// //     // curSTREAM.getVideoTracks().forEach( (camera) => {
// //     //   camera.stop();
// //     // });
// //   })
// // .catch(function(err) {
// //     console.log("An error occured! " + err)
// // })
    
//   })

  // var move=0
  // var flagc = 0
  
  // video.width  = video.videoWidth
  //   video.height = video.videoHeight
   

  // setInterval(() => {
    
  //   // if(move )
  //   // console.log(video.videoWidth)
  //   // console.log(video.width)
  //   // ctx.drawImage(video, 0, 0, video.width, video.height, 0, 0, movie.width, movie.height)
  //   // console.log(video.videoWidth)
  //   // console.log(video.width)
  //   // console.log(movie.width)
  //   // console.log(movie.height)
  //   // ctx.fillStyle="red";
  //   // ctx.arc(move/5,cou,20,20);
  //   // move++
  // },0)

  })

</script>
</body>
</html>